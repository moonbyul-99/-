{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 相和歌辞。铜雀妓:昔年分鼎地，今日望陵台。一旦雄图尽，千秋遗令开。绮罗君不见，歌舞妾空来。恩共漳河水，东流无重回。\n",
      "\n",
      "2000 奉和过慈恩寺应制:凤阙邻金地，龙旂拂宝台。云楣将叶并，风牖送花来。月宫清晚桂，虹梁绚早梅。梵境留宸瞩，掞发丽天才。\n",
      "\n",
      "3000 歌:汉帝临汾水，周仙去洛滨。郢中吟白雪，梁上绕飞尘。响发行云驻，声随子夜新。愿君听扣角，当自识贤臣。\n",
      "\n",
      "4000 石门别杨六钦望:燕人同窜越，万里自相哀。影响无期会，江山此地来。暮年伤泛梗，累日慰寒灰。潮水东南落，浮云西北回。俱看石门远，倚棹两悲哉。\n",
      "\n",
      "5000 别綦毋潜:端笏明光宫，历稔朝云陛。诏刊延阁书，高议平津邸。适意偶轻人，虚心削繁礼。盛得江左风，弥工建安体。高张多绝弦，截河有清济。严冬爽群木，伊洛方清泚。渭水冰下流，潼关雪中启。荷蓧几时还，尘缨待君洗。\n",
      "\n",
      "6000 送集贤学士伊阙史少府敕放归江东觐省（一作綦毋潜诗）:墨客钟张侣，材高吴越珍。千门来谒帝，驷马去荣亲。吏邑沿清洛，乡山指白蘋。归期应不远，当及未央春。\n",
      "\n",
      "7000 梁园吟:我浮黄云去京阙，挂席欲进波连山。天长水阔厌远涉，访古始及平台间。平台为客忧思多，对酒遂作梁园歌。却忆蓬池阮公咏，因吟渌水扬洪波。洪波浩荡迷旧国，路远西归安可得。人生达命岂暇愁，且饮美酒登高楼。平头奴子摇大扇，五月不热疑清秋。玉盘杨梅为君设，吴盐如花皎白雪。持盐把酒但饮之，莫学夷齐事高洁。昔人豪贵信陵君，今人耕种信陵坟。荒城虚照碧山月，古木尽入苍梧云。梁王宫阙今安在，枚马先归不相待。舞影歌声散绿池，空馀汴水东流海。沉吟此事泪满衣，黄金买醉未能归。连呼五白行六博，分曹赌酒酣驰辉。歌且谣，意方远。东山高卧时起来，欲济苍生未应晚。\n",
      "\n",
      "8000 晚登郡阁:怅然高阁望，已掩东城关。春风偏送柳，夜景欲沉山。\n",
      "\n",
      "9000 赠别王十七管记:故交吾未测，薄宦空年岁。晚节踪曩贤，雄词冠当世。堂中皆食客，门外多酒债。产业曾未言，衣裘与人敝。飘飖戎幕下，出入关山际。转战轻壮心，立谈有边计。云沙自回合，天海空迢递。星空汉将骄，月盛胡兵锐。沙深冷陉断，雪暗辽阳闭。亦谓扫欃枪，旋惊陷蜂虿。归旌告东捷，斗骑传西败。遥飞绝汉书，已筑长安第。画龙俱在叶，宠鹤先居卫。勿辞部曲勋，不藉将军势。相逢季冬月，怅望穷海裔。折剑留赠人，严装遂云迈。我行将悠缅，及此还羁滞。曾非济代谋，且有临深诫。随波混清浊，与物同丑丽。眇忆青岩栖，宁忘褐衣拜。自言爱水石，本欲亲兰蕙。何意薄松筠，翻然重菅蒯。恒深取与分，孰慢平生契。款曲鸡黍期，酸辛别离袂。逢时愧名节，遇坎悲沦替。适赵非解纷，游燕往无说。浩歌方振荡，逸翮思凌励。倏若异鹏抟，吾当学蝉蜕。\n",
      "\n",
      "10000 东屯月夜:抱疾漂萍老，防边旧谷屯。春农亲异俗，岁月在衡门。青女霜枫重，黄牛峡水喧。泥留虎斗迹，月挂客愁村。乔木澄稀影，轻云倚细根。数惊闻雀噪，暂睡想猿蹲。日转东方白，风来北斗昏。天寒不成寝，无梦寄归魂。\n",
      "\n",
      "11000 送寿州陈录事:寿阳南渡口，敛笏见诸侯。五两楚云暮，千家淮水秋。开帘对芳草，送客上春洲。请问山中桂，王孙几度游。\n",
      "\n",
      "12000 宫词五首:禁柳烟中闻晓乌，风吹玉漏尽铜壶。内官先向蓬莱殿，金合开香泻御炉。玉楼天半起笙歌，风送宫嫔笑语和。月殿影开闻夜漏，水精帘卷近银河。玉阶容卫宿千官，风猎青旂晓仗寒。侍女先来荐琼蕊，露浆新下九霄盘。九重天乐降神仙，步舞分行踏锦筵。嘈囋一声钟鼓歇，万人楼下拾金钱。金吾持戟护新檐，天乐声传万姓瞻。楼上美人相倚看，红妆透出水精帘。\n",
      "\n",
      "13000 重同畅当奘公院闻琴:误以音声祈远公，请将徽轸付秋风。漾漾硖流吹不尽，月华如在白波中。\n",
      "\n",
      "14000 春词:红烟满户日照梁，天丝软弱虫飞扬。菱花霍霍绕帷光，美人对镜著衣裳。庭中并种相思树，夜夜还栖双凤凰。\n",
      "\n",
      "15000 感寓:残雨倦欹枕，病中时序分。秋虫与秋叶，一夜隔窗闻。虚室对摇落，晤言无与群。冥心试观化，世故如丝棼。但看鸢戾天，岂见山出云。下里徒击节，朱弦秘南薰。梧桐秀朝阳，上有威凤文。终待九成奏，来仪瑞吾君。\n",
      "\n",
      "16000 潭州泊船呈诸公:夜寒眠半觉，鼓笛闹嘈嘈。暗浪舂楼堞，惊风破竹篙。主人看使范，客子读离骚。闻道松醪贱，何须吝错刀。\n",
      "\n",
      "17000 赴和州于武昌县再遇毛仙翁十八兄因成一绝:武昌山下蜀江东，重向仙舟见葛洪。又得案前亲礼拜，大罗天诀玉函封。\n",
      "\n",
      "18000 闲游（一作题山寺僧院）:终日不离尘土间，若为能见此身闲。今朝暂共游僧语，更恨趋时别旧山。\n",
      "\n",
      "19000 有木诗八首:有木名弱柳，结根近清池。风烟借颜色，雨露助华滋。峨峨白雪花，袅袅青丝枝。渐密阴自庇，转高梢四垂。截枝扶为杖，软弱不自持。折条用樊圃，柔脆非其宜。为树信可玩，论材何所施。可惜金堤地，栽之徒尔为。有木名樱桃，得地早滋茂。叶密独承日，花繁偏受露。迎风闇摇动，引鸟潜来去。鸟啄子难成，风来枝莫住。低软易攀玩，佳人屡回顾。色求桃李饶，心向松筠妒。好是映墙花，本非当轩树。所以姓萧人，曾为伐樱赋。有木秋不凋，青青在江北。谓为洞庭橘，美人自移植。上受顾盼恩，下勤浇溉力。实成乃是枳，臭苦不堪食。物有似是者，真伪何由识。美人默无言，对之长叹息。中含害物意，外矫凌霜色。仍向枝叶间，潜生刺如棘。有木名杜梨，阴森覆丘壑。心蠹已空朽，根深尚盘薄。狐媚言语巧，鸟妖声音恶。凭此为巢穴，往来互栖托。四傍五六本，叶枝相交错。借问因何生，秋风吹子落。为长社坛下，无人敢芟斫。几度野火来，风回烧不著。有木香苒苒，山头生一蕟。主人不知名，移种近轩闼。爱其有芳味，因以调麹糵。前后曾饮者，十人无一活。岂徒悔封植，兼亦误采掇。试问识药人，始知名野葛。年深已滋蔓，刀斧不可伐。何时猛风来，为我连根拔。有木名水柽，远望青童童。根株非劲挺，柯叶多蒙笼。彩翠色如柏，鳞皴皮似松。为同松柏类，得列嘉树中。枝弱不胜雪，势高常惧风。雪压低还举，风吹西复东。柔芳甚杨柳，早落先梧桐。惟有一堪赏，中心无蠹虫。有木名凌霄，擢秀非孤标。偶依一株树，遂抽百尺条。托根附树身，开花寄树梢。自谓得其势，无因有动摇。一旦树摧倒，独立暂飘飖。疾风从东起，吹折不终朝。朝为拂云花，暮为委地樵。寄言立身者，勿学柔弱苗。有木名丹桂，四时香馥馥。花团夜雪明，叶翦春云绿。风影清似水，霜枝冷如玉。独占小山幽，不容凡鸟宿。匠人爱芳直，裁截为厦屋。干细力未成，用之君自速。重任虽大过，直心终不曲。纵非梁栋材，犹胜寻常木。\n",
      "\n",
      "20000 独眠吟二首:夜长无睡起阶前，寥落星河欲曙天。十五年来明月夜，何曾一夜不孤眠。独眠客，夜夜可怜长寂寂。就中今夜最愁人，凉月清风满床席。\n",
      "\n",
      "21000 读老子:言者不如知者默，此语吾闻于老君。若道老君是知者，缘何自著五千文。\n",
      "\n",
      "22000 庐山瀑布:虚空落泉千仞直，雷奔入江不暂息。今古长如白练飞，一条界破青山色。\n",
      "\n",
      "23000 仙客归乡词二首:六合八荒游未半，子孙零落暂归来。井边不认捎云树，多是门人在后栽。洞中日月洞中仙，不算离家是几年。出郭始知人代变，又须抛却古时钱。\n",
      "\n",
      "24000 乐静:引手强篸巾，徐徐起病身。远心群野鹤，闲话对村人。发匣琴徽静，开瓶酒味真。纵闻兵赋急，原宪本家贫。\n",
      "\n",
      "25000 送王十至褒中因寄尚书:阙下经年别，人间两地情。坛场新汉将，烟月古隋城。雁去梁山远，云高楚岫明。君家荷藕好，缄恨寄遥程。\n",
      "\n",
      "26000 到秋:扇风淅沥簟流离，万里南风滞所思。守到清秋还寂寞，叶丹苔碧闭门时。\n",
      "\n",
      "27000 送友人游边:方春到帝京，有恋有愁并。万里江海思，半年沙塞程。绿阴斜向驿，残照远侵城。自可资新课，还期振盛名。\n",
      "\n",
      "28000 荒斋:草合径微微，终南对掩扉。晚凉疏雨绝，初晓远山稀。落叶无青地，闲身著白衣。朴愚犹本性，不是学忘机。\n",
      "\n",
      "29000 答韩中丞容不饮酒:老大成名仍足病，强听丝竹亦无欢。高情太守容闲坐，借与青山尽日看。\n",
      "\n",
      "30000 闻开元寺开笋园寄章上人:园锁开声骇鹿群，满林鲜箨水犀文。森森竞泫林梢雨，木雙木雙争穿石上云。并出亦如鹅管合，各生还似犬牙分。折烟束露如相遗，何胤明朝不茹荤。\n",
      "\n",
      "31000 题玄哲禅师影堂:吾师视化身，一念即遗尘。岩谷藏虚塔，江湖散学人。云迷禅处石，院掩写来真。寂寞焚香后，闲阶细草生。\n",
      "\n",
      "32000 柳絮:处处东风扑晚阳，轻轻醉粉落无香。就中堪恨隋堤上，曾惹龙舟舞凤凰。\n",
      "\n",
      "33000 十日菊:节去蜂愁蝶不知，晓庭还绕折残枝。自缘今日人心别，未必秋香一夜衰。\n",
      "\n",
      "34000 送人游江南:满酌劝君酒，劝君君莫辞。能禁几度别，即到白头时。晚岫无云蔽，春帆有燕随。男儿两行泪，不欲等闲垂。\n",
      "\n",
      "35000 和吴学士对春雪献韦令公次韵:春雪下盈空，翻疑腊未穷。连天宁认月，堕地屡兼风。忽误边沙上，应平火岭中。林间妨走兽，云际落飞鸿。梁苑还吟客，齐都省创宫。掩扉皆墐北，移律愧居东。书幌飘全湿，茶铛入旋融。奔川半留滞，叠树互玲珑。出户行瑶砌，开园见粉丛。高才兴咏处，真宰答殊功。\n",
      "\n",
      "36000 三代门。又吟:千妖万态逞妍姿，破国亡家更是谁。匡政必能除苟媚，去邪当断勿狐疑。\n",
      "\n",
      "37000 走笔送义兴令赵宣辅:闻君孤棹泛荆溪，陇首云随别恨飞。杜牧旧居凭买取，他年藜杖愿同归。\n",
      "\n",
      "38000 日暖万年枝:旭日升溟海，芳枝散曙烟。温仁临树久，煦妪在条偏。阳德符君惠，嘉名表圣年。若承恩渥厚，常属栋梁贤。生植虽依地，光华只信天。不才堪仄陋，徒望向荣先。\n",
      "\n",
      "39000 江行寄张舍人:客程终日风尘苦，蓬转还家未有期。林色晓分残雪后，角声寒奏落帆时。月高星使东看远，云破霜鸿北度迟。流荡此心难共说，千峰澄霁隔琼枝。\n",
      "\n",
      "40000 离乱后寄九峰和尚二首:乱后知深隐，庵应近石楼。异香因雪歇，仙果落池浮。诗老全抛格，心空未到头。还应嫌笑我，世路独悠悠。萧洒复萧洒，松根独据梧。瀑冰吟次折，远烧坐来无。老□寒披衲，孤云静入厨。不知知我否，已到不区区。\n",
      "\n",
      "41000 题玉泉寺:高韵双悬张曲江，联题兼是孟襄阳。后人才地谁称短，前辈经天尽负长。胜景饱于闲采拾，灵踪销得正思量。时移两板成尘迹，犹挂吾师旧影堂。\n",
      "\n",
      "42000 嘲崔垂休:慈恩塔下亲泥壁，滑腻光华玉不如。何事博陵崔四十，金陵腿上逞欧书。\n",
      "\n",
      "43000 菩萨蛮:牡丹含露真珠颗，美人折向庭前过。含笑问檀郎，花强妾貌强？檀郎故相恼，须道花枝好。一面发娇嗔，碎挪花打人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"poetry.txt\")               # 返回一个文件对象   \n",
    "line = f.readline() \n",
    "count = 0              # 调用文件的 readline()方法，一次读取一行\n",
    "while line:   \n",
    "    count += 1                 # 后面跟 ',' 将忽略换行符   \n",
    "    line = f.readline()   \n",
    "    if count % 1000 == 0:\n",
    "        print(count, line)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "class Tokenizer:\n",
    "    def __init__(self, token2id_dict):\n",
    "        self.token2id_dict = token2id_dict\n",
    "        self.id2token_dict = {}\n",
    "        for key, value in self.token2id_dict.items():\n",
    "            self.id2token_dict[value] = key\n",
    "        self.vocab_size = len(self.token2id_dict)\n",
    "    \n",
    "    def id2token(self, id):\n",
    "        return self.id2token_dict[id]\n",
    "    \n",
    "    def token2id(self, token):\n",
    "        if token in self.token2id_dict.keys():\n",
    "            return self.token2id_dict[token]\n",
    "        else:\n",
    "            return self.token2id_dict['[UNK]']\n",
    "    \n",
    "    def encode(self, s):\n",
    "        id_list = [self.token2id_dict['[CLS]']]\n",
    "        for token in s:\n",
    "            id_list.append(self.token2id(token))\n",
    "        id_list.append(self.token2id_dict['[SEP]'])\n",
    "        return id_list\n",
    "    \n",
    "    def decode(self,id_list):\n",
    "        special_token = ['[CLS]','[SEP]']\n",
    "        s = []\n",
    "        for id in id_list:\n",
    "            token = self.id2token(id)\n",
    "            if token in special_token:\n",
    "                continue\n",
    "            s.append(token)\n",
    "        return ''.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(text,L):\n",
    "    if len(text) > L:\n",
    "        res = list(text[:L])\n",
    "    else:\n",
    "        res = list(text) + ['[PAD]']*(L-len(text))\n",
    "    return res\n",
    "        \n",
    "def get_id_list(L = 64, filepath = 'poetry.txt'):\n",
    "    f = open(filepath) \n",
    "\n",
    "    data_list = []\n",
    "    token_count_dict = {}           \n",
    "    line = f.readline() \n",
    "\n",
    "    count = 0\n",
    "    while line:  \n",
    "        line = f.readline()\n",
    "        if '：' in line:\n",
    "            line = line.replace('：',':')\n",
    "        if line.count(':') != 1:\n",
    "            continue\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "            \n",
    "        main_part = line.split(':')[1]\n",
    "        data_list.append(main_part)\n",
    "        token_list = list(main_part)\n",
    "        for token in token_list:\n",
    "            if token in token_count_dict.keys():\n",
    "                token_count_dict[token] += 1\n",
    "            else:\n",
    "                token_count_dict[token]  = 1\n",
    "    f.close() \n",
    "\n",
    "    _tokens = [(token, count) for token, count in token_count_dict.items() if count >= 4]\n",
    "    # 按词频排序\n",
    "    _tokens = sorted(_tokens, key=lambda x: -x[1])\n",
    "\n",
    "    token2id_dict = {}\n",
    "    token2id_dict['[UNK]'] = 0\n",
    "    token2id_dict['[CLS]'] = 1\n",
    "    token2id_dict['[SEP]'] = 2\n",
    "    token2id_dict['[PAD]'] = 3\n",
    "    for i,_ in enumerate(_tokens):\n",
    "        token2id_dict[_[0]] = i+4\n",
    "    \n",
    "    tokenizer = Tokenizer(token2id_dict)\n",
    "\n",
    "    id_list = []\n",
    "    for text in data_list:\n",
    "        data = padding(text,L)\n",
    "        id = tokenizer.encode(data)\n",
    "        id_list.append(id)\n",
    "    \n",
    "    id_list = np.array(id_list)\n",
    "    id_list = torch.from_numpy(id_list)\n",
    "    return id_list, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list, tokenizer = get_id_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "def text_data_loader(id_list, batch_size, shuffle=True):\n",
    "    data_loader = DataLoader(dataset= id_list,batch_size= batch_size,shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([64, 66])\n",
      "torch.Size([26, 66])\n"
     ]
    }
   ],
   "source": [
    "data_loader = text_data_loader(id_list, 64)\n",
    "for i, batch in enumerate(data_loader):\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "embed = nn.Embedding(num_embeddings=tokenizer.vocab_size, embedding_dim = 128)\n",
    "out = embed(batch.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class mymodel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim = 128, hidden_dim_1 = 64, hidden_dim_2 = 64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm_1 = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim_1, num_layers = 2, dropout = 0.3)\n",
    "        self.lstm_2 = nn.LSTM(input_size = hidden_dim_1, hidden_size = hidden_dim_2, num_layers = 1, dropout = 0.2)\n",
    "        self.mlp = nn.Linear(in_features=hidden_dim_2, out_features= num_embeddings)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm_1(x)\n",
    "        x, _ = self.lstm_2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim = 256, hidden_dim_1 = 128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm_1 = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim_1, num_layers = 1)\n",
    "        self.mlp = nn.Linear(in_features=hidden_dim_1, out_features= num_embeddings)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm_1(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  196,  366,  ...,    3,    3,    2],\n",
       "        [   1,   11,   75,  ..., 2067,  116,    2],\n",
       "        [   1,  660, 1208,  ...,    3,    3,    2],\n",
       "        ...,\n",
       "        [   1,  455, 1110,  ...,   18,  220,    2],\n",
       "        [   1,   44,   73,  ...,    3,    3,    2],\n",
       "        [   1,  581,  125,  ...,    3,    3,    2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 epoch_loss 8.6497\n",
      "epoch 0 batch 50 epoch_loss 435.4095\n",
      "epoch 0 batch 100 epoch_loss 857.3989\n",
      "epoch 0 batch 150 epoch_loss 1279.2574\n",
      "epoch 0 batch 200 epoch_loss 1701.0292\n",
      "epoch 0 batch 250 epoch_loss 2120.586\n",
      "epoch 0 batch 300 epoch_loss 2539.5625\n",
      "epoch 0 batch 350 epoch_loss 2958.6485\n",
      "epoch 0 batch 400 epoch_loss 3377.8302\n",
      "epoch 0 batch 450 epoch_loss 3796.4996\n",
      "epoch 0 batch 500 epoch_loss 4215.7499\n",
      "epoch 0 batch 550 epoch_loss 4634.32\n",
      "epoch 0 batch 600 epoch_loss 5053.2158\n",
      "epoch 0 batch 650 epoch_loss 5472.042\n",
      "epoch 1 batch 0 epoch_loss 8.3546\n",
      "epoch 1 batch 50 epoch_loss 427.6257\n",
      "epoch 1 batch 100 epoch_loss 846.606\n",
      "epoch 1 batch 150 epoch_loss 1265.6815\n",
      "epoch 1 batch 200 epoch_loss 1684.4838\n",
      "epoch 1 batch 250 epoch_loss 2103.4196\n",
      "epoch 1 batch 300 epoch_loss 2521.8487\n",
      "epoch 1 batch 350 epoch_loss 2940.9628\n",
      "epoch 1 batch 400 epoch_loss 3359.9582\n",
      "epoch 1 batch 450 epoch_loss 3778.9471\n",
      "epoch 1 batch 500 epoch_loss 4197.6743\n",
      "epoch 1 batch 550 epoch_loss 4616.5973\n",
      "epoch 1 batch 600 epoch_loss 5035.789\n",
      "epoch 1 batch 650 epoch_loss 5454.7614\n",
      "epoch 2 batch 0 epoch_loss 8.373\n",
      "epoch 2 batch 50 epoch_loss 427.1784\n",
      "epoch 2 batch 100 epoch_loss 846.2937\n",
      "epoch 2 batch 150 epoch_loss 1265.4155\n",
      "epoch 2 batch 200 epoch_loss 1684.7097\n",
      "epoch 2 batch 250 epoch_loss 2103.7307\n",
      "epoch 2 batch 300 epoch_loss 2522.9084\n",
      "epoch 2 batch 350 epoch_loss 2941.6185\n",
      "epoch 2 batch 400 epoch_loss 3360.3288\n",
      "epoch 2 batch 450 epoch_loss 3778.8712\n",
      "epoch 2 batch 500 epoch_loss 4197.8988\n",
      "epoch 2 batch 550 epoch_loss 4616.8076\n",
      "epoch 2 batch 600 epoch_loss 5035.4723\n",
      "epoch 2 batch 650 epoch_loss 5454.284\n",
      "epoch 3 batch 0 epoch_loss 8.356\n",
      "epoch 3 batch 50 epoch_loss 427.1994\n",
      "epoch 3 batch 100 epoch_loss 846.2506\n",
      "epoch 3 batch 150 epoch_loss 1265.2958\n",
      "epoch 3 batch 200 epoch_loss 1683.8776\n",
      "epoch 3 batch 250 epoch_loss 2102.8252\n",
      "epoch 3 batch 300 epoch_loss 2521.936\n",
      "epoch 3 batch 350 epoch_loss 2940.4134\n",
      "epoch 3 batch 400 epoch_loss 3359.5133\n",
      "epoch 3 batch 450 epoch_loss 3778.3254\n",
      "epoch 3 batch 500 epoch_loss 4197.3319\n",
      "epoch 3 batch 550 epoch_loss 4616.1896\n",
      "epoch 3 batch 600 epoch_loss 5035.2532\n",
      "epoch 3 batch 650 epoch_loss 5454.0516\n",
      "epoch 4 batch 0 epoch_loss 8.3908\n",
      "epoch 4 batch 50 epoch_loss 427.29\n",
      "epoch 4 batch 100 epoch_loss 846.0566\n",
      "epoch 4 batch 150 epoch_loss 1264.8594\n",
      "epoch 4 batch 200 epoch_loss 1683.7108\n",
      "epoch 4 batch 250 epoch_loss 2102.5044\n",
      "epoch 4 batch 300 epoch_loss 2521.5588\n",
      "epoch 4 batch 350 epoch_loss 2940.4746\n",
      "epoch 4 batch 400 epoch_loss 3359.3956\n",
      "epoch 4 batch 450 epoch_loss 3778.1991\n",
      "epoch 4 batch 500 epoch_loss 4197.0498\n",
      "epoch 4 batch 550 epoch_loss 4615.8379\n",
      "epoch 4 batch 600 epoch_loss 5035.0684\n",
      "epoch 4 batch 650 epoch_loss 5453.6478\n",
      "epoch 5 batch 0 epoch_loss 8.3603\n",
      "epoch 5 batch 50 epoch_loss 427.2664\n",
      "epoch 5 batch 100 epoch_loss 846.0249\n",
      "epoch 5 batch 150 epoch_loss 1264.6292\n",
      "epoch 5 batch 200 epoch_loss 1683.666\n",
      "epoch 5 batch 250 epoch_loss 2102.7485\n",
      "epoch 5 batch 300 epoch_loss 2521.5636\n",
      "epoch 5 batch 350 epoch_loss 2940.6532\n",
      "epoch 5 batch 400 epoch_loss 3359.6781\n",
      "epoch 5 batch 450 epoch_loss 3778.6554\n",
      "epoch 5 batch 500 epoch_loss 4197.3354\n",
      "epoch 5 batch 550 epoch_loss 4616.3416\n",
      "epoch 5 batch 600 epoch_loss 5034.9787\n",
      "epoch 5 batch 650 epoch_loss 5453.6311\n",
      "epoch 6 batch 0 epoch_loss 8.3836\n",
      "epoch 6 batch 50 epoch_loss 427.2095\n",
      "epoch 6 batch 100 epoch_loss 845.9429\n",
      "epoch 6 batch 150 epoch_loss 1264.839\n",
      "epoch 6 batch 200 epoch_loss 1683.4954\n",
      "epoch 6 batch 250 epoch_loss 2102.1438\n",
      "epoch 6 batch 300 epoch_loss 2521.0519\n",
      "epoch 6 batch 350 epoch_loss 2939.8348\n",
      "epoch 6 batch 400 epoch_loss 3358.5698\n",
      "epoch 6 batch 450 epoch_loss 3777.3736\n",
      "epoch 6 batch 500 epoch_loss 4196.3803\n",
      "epoch 6 batch 550 epoch_loss 4615.1601\n",
      "epoch 6 batch 600 epoch_loss 5033.8587\n",
      "epoch 6 batch 650 epoch_loss 5452.984\n",
      "epoch 7 batch 0 epoch_loss 8.3312\n",
      "epoch 7 batch 50 epoch_loss 427.1195\n",
      "epoch 7 batch 100 epoch_loss 846.0851\n",
      "epoch 7 batch 150 epoch_loss 1264.5545\n",
      "epoch 7 batch 200 epoch_loss 1683.3061\n",
      "epoch 7 batch 250 epoch_loss 2102.2291\n",
      "epoch 7 batch 300 epoch_loss 2520.9573\n",
      "epoch 7 batch 350 epoch_loss 2939.7514\n",
      "epoch 7 batch 400 epoch_loss 3358.8251\n",
      "epoch 7 batch 450 epoch_loss 3777.6338\n",
      "epoch 7 batch 500 epoch_loss 4194.1799\n",
      "epoch 7 batch 550 epoch_loss 4610.3052\n",
      "epoch 7 batch 600 epoch_loss 5026.6694\n",
      "epoch 7 batch 650 epoch_loss 5442.6474\n",
      "epoch 8 batch 0 epoch_loss 8.3036\n",
      "epoch 8 batch 50 epoch_loss 424.3102\n",
      "epoch 8 batch 100 epoch_loss 840.3285\n",
      "epoch 8 batch 150 epoch_loss 1256.1508\n",
      "epoch 8 batch 200 epoch_loss 1671.8728\n",
      "epoch 8 batch 250 epoch_loss 2088.3469\n",
      "epoch 8 batch 300 epoch_loss 2504.306\n",
      "epoch 8 batch 350 epoch_loss 2920.4041\n",
      "epoch 8 batch 400 epoch_loss 3336.4136\n",
      "epoch 8 batch 450 epoch_loss 3752.4631\n",
      "epoch 8 batch 500 epoch_loss 4168.6543\n",
      "epoch 8 batch 550 epoch_loss 4584.8176\n",
      "epoch 8 batch 600 epoch_loss 5000.6358\n",
      "epoch 8 batch 650 epoch_loss 5416.7856\n",
      "epoch 9 batch 0 epoch_loss 8.3281\n",
      "epoch 9 batch 50 epoch_loss 424.335\n",
      "epoch 9 batch 100 epoch_loss 840.3494\n",
      "epoch 9 batch 150 epoch_loss 1256.6581\n",
      "epoch 9 batch 200 epoch_loss 1672.8749\n",
      "epoch 9 batch 250 epoch_loss 2088.8487\n",
      "epoch 9 batch 300 epoch_loss 2504.6684\n",
      "epoch 9 batch 350 epoch_loss 2920.8046\n",
      "epoch 9 batch 400 epoch_loss 3336.9249\n",
      "epoch 9 batch 450 epoch_loss 3753.039\n",
      "epoch 9 batch 500 epoch_loss 4168.5001\n",
      "epoch 9 batch 550 epoch_loss 4584.3558\n",
      "epoch 9 batch 600 epoch_loss 5000.6125\n",
      "epoch 9 batch 650 epoch_loss 5416.6545\n",
      "epoch 10 batch 0 epoch_loss 8.3261\n",
      "epoch 10 batch 50 epoch_loss 424.1529\n",
      "epoch 10 batch 100 epoch_loss 839.7684\n",
      "epoch 10 batch 150 epoch_loss 1255.8929\n",
      "epoch 10 batch 200 epoch_loss 1671.8353\n",
      "epoch 10 batch 250 epoch_loss 2087.8843\n",
      "epoch 10 batch 300 epoch_loss 2504.069\n",
      "epoch 10 batch 350 epoch_loss 2920.0416\n",
      "epoch 10 batch 400 epoch_loss 3336.1846\n",
      "epoch 10 batch 450 epoch_loss 3752.0021\n",
      "epoch 10 batch 500 epoch_loss 4168.1227\n",
      "epoch 10 batch 550 epoch_loss 4584.2523\n",
      "epoch 10 batch 600 epoch_loss 5000.3786\n",
      "epoch 10 batch 650 epoch_loss 5416.5622\n",
      "epoch 11 batch 0 epoch_loss 8.3373\n",
      "epoch 11 batch 50 epoch_loss 424.4681\n",
      "epoch 11 batch 100 epoch_loss 840.4511\n",
      "epoch 11 batch 150 epoch_loss 1256.3021\n",
      "epoch 11 batch 200 epoch_loss 1672.2422\n",
      "epoch 11 batch 250 epoch_loss 2088.1398\n",
      "epoch 11 batch 300 epoch_loss 2504.4191\n",
      "epoch 11 batch 350 epoch_loss 2920.4376\n",
      "epoch 11 batch 400 epoch_loss 3336.3458\n",
      "epoch 11 batch 450 epoch_loss 3752.3292\n",
      "epoch 11 batch 500 epoch_loss 4168.5549\n",
      "epoch 11 batch 550 epoch_loss 4584.4385\n",
      "epoch 11 batch 600 epoch_loss 5000.4165\n",
      "epoch 11 batch 650 epoch_loss 5416.5928\n",
      "epoch 12 batch 0 epoch_loss 8.3113\n",
      "epoch 12 batch 50 epoch_loss 424.2959\n",
      "epoch 12 batch 100 epoch_loss 840.1927\n",
      "epoch 12 batch 150 epoch_loss 1256.4073\n",
      "epoch 12 batch 200 epoch_loss 1672.4816\n",
      "epoch 12 batch 250 epoch_loss 2088.38\n",
      "epoch 12 batch 300 epoch_loss 2504.2578\n",
      "epoch 12 batch 350 epoch_loss 2920.3855\n",
      "epoch 12 batch 400 epoch_loss 3336.3309\n",
      "epoch 12 batch 450 epoch_loss 3752.3759\n",
      "epoch 12 batch 500 epoch_loss 4168.5114\n",
      "epoch 12 batch 550 epoch_loss 4584.6077\n",
      "epoch 12 batch 600 epoch_loss 5000.582\n",
      "epoch 12 batch 650 epoch_loss 5416.6551\n",
      "epoch 13 batch 0 epoch_loss 8.3307\n",
      "epoch 13 batch 50 epoch_loss 424.5005\n",
      "epoch 13 batch 100 epoch_loss 840.564\n",
      "epoch 13 batch 150 epoch_loss 1256.4524\n",
      "epoch 13 batch 200 epoch_loss 1672.505\n",
      "epoch 13 batch 250 epoch_loss 2088.4605\n",
      "epoch 13 batch 300 epoch_loss 2504.4257\n",
      "epoch 13 batch 350 epoch_loss 2920.5577\n",
      "epoch 13 batch 400 epoch_loss 3336.4535\n",
      "epoch 13 batch 450 epoch_loss 3752.5278\n",
      "epoch 13 batch 500 epoch_loss 4168.2494\n",
      "epoch 13 batch 550 epoch_loss 4584.4519\n",
      "epoch 13 batch 600 epoch_loss 5000.3599\n",
      "epoch 13 batch 650 epoch_loss 5416.6239\n",
      "epoch 14 batch 0 epoch_loss 8.3163\n",
      "epoch 14 batch 50 epoch_loss 424.4116\n",
      "epoch 14 batch 100 epoch_loss 840.3686\n",
      "epoch 14 batch 150 epoch_loss 1256.6247\n",
      "epoch 14 batch 200 epoch_loss 1672.4898\n",
      "epoch 14 batch 250 epoch_loss 2088.4465\n",
      "epoch 14 batch 300 epoch_loss 2504.4314\n",
      "epoch 14 batch 350 epoch_loss 2920.5471\n",
      "epoch 14 batch 400 epoch_loss 3336.7564\n",
      "epoch 14 batch 450 epoch_loss 3752.6961\n",
      "epoch 14 batch 500 epoch_loss 4168.7146\n",
      "epoch 14 batch 550 epoch_loss 4584.5321\n",
      "epoch 14 batch 600 epoch_loss 5000.4779\n",
      "epoch 14 batch 650 epoch_loss 5416.5952\n",
      "epoch 15 batch 0 epoch_loss 8.3694\n",
      "epoch 15 batch 50 epoch_loss 424.2417\n",
      "epoch 15 batch 100 epoch_loss 840.4121\n",
      "epoch 15 batch 150 epoch_loss 1256.5572\n",
      "epoch 15 batch 200 epoch_loss 1672.5816\n",
      "epoch 15 batch 250 epoch_loss 2088.4499\n",
      "epoch 15 batch 300 epoch_loss 2504.4481\n",
      "epoch 15 batch 350 epoch_loss 2920.2773\n",
      "epoch 15 batch 400 epoch_loss 3336.5239\n",
      "epoch 15 batch 450 epoch_loss 3752.3228\n",
      "epoch 15 batch 500 epoch_loss 4168.181\n",
      "epoch 15 batch 550 epoch_loss 4584.4331\n",
      "epoch 15 batch 600 epoch_loss 5000.3765\n",
      "epoch 15 batch 650 epoch_loss 5416.3619\n",
      "epoch 16 batch 0 epoch_loss 8.3391\n",
      "epoch 16 batch 50 epoch_loss 423.986\n",
      "epoch 16 batch 100 epoch_loss 840.1127\n",
      "epoch 16 batch 150 epoch_loss 1256.1836\n",
      "epoch 16 batch 200 epoch_loss 1672.0717\n",
      "epoch 16 batch 250 epoch_loss 2088.1914\n",
      "epoch 16 batch 300 epoch_loss 2504.0153\n",
      "epoch 16 batch 350 epoch_loss 2920.0067\n",
      "epoch 16 batch 400 epoch_loss 3336.0363\n",
      "epoch 16 batch 450 epoch_loss 3752.0251\n",
      "epoch 16 batch 500 epoch_loss 4168.213\n",
      "epoch 16 batch 550 epoch_loss 4584.1351\n",
      "epoch 16 batch 600 epoch_loss 5000.0982\n",
      "epoch 16 batch 650 epoch_loss 5416.1709\n",
      "epoch 17 batch 0 epoch_loss 8.292\n",
      "epoch 17 batch 50 epoch_loss 424.3426\n",
      "epoch 17 batch 100 epoch_loss 840.5407\n",
      "epoch 17 batch 150 epoch_loss 1256.5849\n",
      "epoch 17 batch 200 epoch_loss 1672.4114\n",
      "epoch 17 batch 250 epoch_loss 2088.3737\n",
      "epoch 17 batch 300 epoch_loss 2504.6345\n",
      "epoch 17 batch 350 epoch_loss 2920.7493\n",
      "epoch 17 batch 400 epoch_loss 3336.7829\n",
      "epoch 17 batch 450 epoch_loss 3752.6633\n",
      "epoch 17 batch 500 epoch_loss 4168.5402\n",
      "epoch 17 batch 550 epoch_loss 4584.5728\n",
      "epoch 17 batch 600 epoch_loss 5000.6067\n",
      "epoch 17 batch 650 epoch_loss 5416.3997\n",
      "epoch 18 batch 0 epoch_loss 8.3403\n",
      "epoch 18 batch 50 epoch_loss 424.3995\n",
      "epoch 18 batch 100 epoch_loss 840.3365\n",
      "epoch 18 batch 150 epoch_loss 1256.456\n",
      "epoch 18 batch 200 epoch_loss 1672.4803\n",
      "epoch 18 batch 250 epoch_loss 2088.563\n",
      "epoch 18 batch 300 epoch_loss 2504.1766\n",
      "epoch 18 batch 350 epoch_loss 2920.417\n",
      "epoch 18 batch 400 epoch_loss 3336.3617\n",
      "epoch 18 batch 450 epoch_loss 3752.2891\n",
      "epoch 18 batch 500 epoch_loss 4168.0953\n",
      "epoch 18 batch 550 epoch_loss 4584.107\n",
      "epoch 18 batch 600 epoch_loss 5000.1688\n",
      "epoch 18 batch 650 epoch_loss 5416.1499\n",
      "epoch 19 batch 0 epoch_loss 8.3219\n",
      "epoch 19 batch 50 epoch_loss 424.3881\n",
      "epoch 19 batch 100 epoch_loss 840.6008\n",
      "epoch 19 batch 150 epoch_loss 1256.4158\n",
      "epoch 19 batch 200 epoch_loss 1672.4645\n",
      "epoch 19 batch 250 epoch_loss 2087.9881\n",
      "epoch 19 batch 300 epoch_loss 2503.722\n",
      "epoch 19 batch 350 epoch_loss 2919.6463\n",
      "epoch 19 batch 400 epoch_loss 3335.6618\n",
      "epoch 19 batch 450 epoch_loss 3751.8358\n",
      "epoch 19 batch 500 epoch_loss 4168.0129\n",
      "epoch 19 batch 550 epoch_loss 4584.1514\n",
      "epoch 19 batch 600 epoch_loss 5000.3877\n",
      "epoch 19 batch 650 epoch_loss 5416.2905\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "\n",
    "model = mymodel(num_embeddings=vocab_size,  embedding_dim =256, hidden_dim_1= 128)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "id_list = id_list.to(device)\n",
    "model = model.to(device)\n",
    "one_hot_embedding = nn.Embedding(vocab_size, vocab_size, _weight=torch.from_numpy(np.eye(vocab_size))).to(device)\n",
    "\n",
    "epoch_loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    dataloader = text_data_loader(id_list, batch_size=64, shuffle=True)\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[:,:-1].T\n",
    "        y = one_hot_embedding(batch[:,1:])\n",
    "\n",
    "        output = model(x)\n",
    "        output = output.permute([1,2,0])\n",
    "        y = y.permute([0,2,1])\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if i %50 == 0:\n",
    "            print(f'epoch {epoch} batch {i} epoch_loss {round(epoch_loss,4)}')\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_poetry(tokenizer, model, s=''):\n",
    "    \"\"\"\n",
    "    随机生成一首诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param s: 用于生成古诗的起始字符串，默认为空串\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 将初始字符串转成token\n",
    "    token_ids = tokenizer.encode(s)\n",
    "    # 去掉结束标记[SEP]\n",
    "    token_ids = token_ids[:-1]\n",
    "    while len(token_ids) < 66:\n",
    "        # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "        output = model(np.array([token_ids, ], dtype=np.int32))\n",
    "        _probas = output.numpy()[0, -1, 3:]\n",
    "        del output\n",
    "        # print(_probas)\n",
    "        # 按照出现概率，对所有token倒序排列\n",
    "        p_args = _probas.argsort()[::-1][:100]\n",
    "        # 排列后的概率顺序\n",
    "        p = _probas[p_args]\n",
    "        # 先对概率归一\n",
    "        p = p / sum(p)\n",
    "        # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "        target_index = np.random.choice(len(p), p=p)\n",
    "        target = p_args[target_index] + 3\n",
    "        # 保存\n",
    "        token_ids.append(target)\n",
    "        if target == 3:\n",
    "            break\n",
    "    return tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "token_ids = tokenizer.encode(s)\n",
    "token_ids = token_ids[:-1]\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mymodel(\n",
       "  (embedding): Embedding(5708, 256)\n",
       "  (lstm_1): LSTM(256, 128)\n",
       "  (mlp): Linear(in_features=128, out_features=5708, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [88,90]\n",
    "while len(token_ids) < 66:\n",
    "    # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "    output = model(torch.from_numpy(np.array([token_ids, ], dtype=np.int32)))\n",
    "    _probas =  output.detach().numpy()[0, -1, 3:]\n",
    "    del output\n",
    "    # print(_probas)\n",
    "    # 按照出现概率，对所有token倒序排列\n",
    "    p_args = _probas.argsort()[::-1][:100]\n",
    "    # 排列后的概率顺序\n",
    "    p = _probas[p_args]\n",
    "    # 先对概率归一\n",
    "    p = p / sum(p)\n",
    "    # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "    target_index = np.random.choice(len(p), p=p)\n",
    "    target = p_args[target_index] + 3\n",
    "    # 保存\n",
    "    token_ids.append(target)\n",
    "    if target == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
